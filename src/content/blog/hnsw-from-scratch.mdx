---
title: "A Visual Guide to Higher Navigational Small World Graphs (HNSW) and How It Makes Vector Search Faster"
description: "Understanding HNSW by implementing a toy example from scratch"
pubDate: "March 20 2024"
heroImage: "/HNSW.jpg"
---

import { DisplayNSWGraph } from "../../components/DisplayNSWGraph";
import {
  NoteDirective,
  WarnDirective,
  Directive,
} from "../../components/NoteDirective";
import { GraphNode } from "../../lib/navigable-small-world/graph-node";
import { SimpleNodesOnGraph } from "../../components/SimpleNodesOnGraph";
import { MultipleLayerGraphWithNodes } from "../../components/MultipleLayerGraphWithNodes";
import { DisplaySkipList } from "../../components/DisplaySkipList";

<WarnDirective text="This article is a work in progress. The code examples and the visualizations are placeholders and will be updated soon." />

<DisplayNSWGraph client:load autoRun={true} />

<article className="prose lg:prose-xl">

This article provides a high-level overview of the Hierarchical Navigable Small World (HNSW) algorithm, a tool for accelerating
similarity search. Despite its initial apparent complexity, understanding HNSW becomes easier when we break it down into its fundamental,
simpler concepts.

In this post, we'll dissect the HNSW algorithm, exploring its underlying principles with interactive visualizations. In a future post, we'll
build a toy implementation of the supporting algorithms and HNSW from scratch.

While this post is designed for readers with minimal technical background, some understanding of basic computer science concepts like data
structures and time complexity will be helpful.

# Why is HNSW Important?

The rise of Generative AI has sparked interest in vector search and databases, particularly their role in Retrieval-Augmented Generation
(RAG) systems. Vector databases enable fast similarity search across numerous documents, aiding large language models (LLMs) in providing
more precise and contextually relevant responses. This is achieved by grounding LLMs with real-world data retrieved by similarity search,
improving their understanding and response accuracy.

This post focuses on the long-standing problem of similarity search in computer science, and how Approximate Nearest Neighbor (ANN)
algorithms, specifically the HNSW algorithm, have enhanced the speed of similarity search in vector databases.

# What to Expect

This post may need to be split into multiple posts, as there are several topics to cover:

- Basics of similarity search and k-nearest neighbors (KNN)
- Introduction to Skip Lists and Navigable Small World (NSW) graphs with interactive demos and code samples
- Implementation of a toy HNSW with interactive visualizations

Even with the above topics, there is still so much to cover in the realm of similarity search that we cannot squeeze into a single post. If you find this article interesting and would like to see similar articles that dive through these other topics not touched on in this post, let me know!

# The Problem of Similarity Search

Similarity search, a process that identifies similar items within a large dataset, is crucial in various applications such as recommendation systems, search engines, and information retrieval systems. This process is often applied to diverse data types, including text documents, images, audio files, and even gifs.

However, comparing these items directly can be complex. Unlike numbers, where you can subtract two to get the difference between them, there is no straightforward way to compare two text documents or images. To remedy this, we can convert these items into their numerical representations, vectors. This conversion allows us to perform mathematical comparisons more straightforwardly.

For example, let's consider the following sentences:

1. "Ham Ham Ham Ham Ham"
2. "Spam Spam Spam Spam Spam"
3. "Ham Ham Spam Spam Spam"

We can compare these sentences by counting the frequency of each word, resulting in the following numerical representations:

1. "<span className="text-red-700">Ham Ham Ham Ham Ham</span>" -> <b>[<span className="text-red-700">5</span>, <span className="text-blue-700">0</span>]</b>
2. "<span className="text-blue-700">Spam Spam Spam Spam Spam</span>" -> <b>[<span className="text-red-700">0</span>, <span className="text-blue-700">5</span>]</b>
   {/* prettier-ignore */}
3. "<span className="text-red-700">Ham Ham </span><span className="text-blue-700">Spam Spam Spam</span>" -> <b>[<span className="text-red-700">2</span>,{" "}<span className="text-blue-700">3</span>]</b>

These numerical vector representations are just a list of numbers, which enables us to now easily compare the sentences by calculating
the distances between them.

</article>

<Directive text="In vector search, a vector is a list of numbers representing a point in a multi-dimensional space. A 2D vector can be represented as `[x, y]`, and a 3D vector as `[x, y, z]`. An n-dimensional vector can be represented as `[x1, x2, ..., xn]`." />

<article className="prose lg:prose-xl">

One standard method to calculate the distance
between vectors is the Euclidean distance, which is the straight-line distance
between two points in a multi-dimensional space. For the two-dimensional vectors we created above,
it's easy for us to visualize the distance between the vectors by plotting the vectors on a 2D plot:

</article>

<SimpleNodesOnGraph client:load />

<article className="prose lg:prose-xl">

It's easy to visually see that the distance between vectors 2 and 3 is less than the distance between vectors 1 and 3, which makes sense. Sentence 3 is more similar to sentence 2 than to sentence 1 because it has more instances of "Spam" than "Ham."

While the Euclidian distance is a simple and intuitive way to measure distance between vectors, it is not always the best choice. High-dimensional spaces can suffer from the "curse of dimensionality," where noisy or unrelated values in the vector can overshadow valuable values, skewing the distance between vectors. This issue can be partially mitigated with data normalization and dimension weighting. However, other distance metrics are less susceptible to these issues.

One such distance metric that is commonly used is Cosine similarity. This can be a highly performant metric, as it is a fast calculation that ignores the magnitude of the vectors and focuses on the direction.

In all of our examples on this page, we use Euclidean distance to simplify visualization.

## Methods to Generate Vectors from Data

There are many techniques to generate vectors from data.

In the above example, we
used the bag-of-words method to generate a vector from a text document, where each
number in the vector represents the frequency of a particular word.

More sophisticated
methods, such as embeddings generated from Large Language Models (LLM) as vectors, are also used.

The choice of which method is used to generate a vector representation depends on the specific use case. Once we have the vectors, we can use them for similarity search.

## Hey Neighbour, Can I Borrow Some Sugar?

As the example above shows, a similarity search with vectors is intuitive. Suppose we plot the vectors in a multi-dimensional space. In that case, similar vectors should be close to each other for any distance metric.

The k-nearest neighbors (KNN) algorithm does this. It calculates the distance between the given vector and all the other vectors in the dataset and returns the k vectors with the smallest distance to the given vector.

Below is an illustration of KNN (with `k` = 1), where we have vector 0 with a vector of [2,3] that we want to find the closest vector to from the following set:

</article>

<SimpleNodesOnGraph
  client:load
  nodes={[
    new GraphNode(1, [6, 1]),
    new GraphNode(2, [5, 6]),
    new GraphNode(3, [5, 4]),
    new GraphNode(4, [1, 5]),
    new GraphNode(5, [0, 1]),
  ]}
  targetNode={new GraphNode(0, [2, 3])}
  k={1}
/>

<article className="prose lg:prose-xl">

We can see that the vector 0 is closest to vector 2, so the KNN algorithm would return vector 2 as the nearest neighbor.

## Is KNN All You Need?

Despite its simplicity and intuitiveness, the k-nearest neighbors (KNN) algorithm has limitations, primarily due to poor performance in high-dimensional and large-scale datasets.

To touch on algorithmic complexity in the naive KNN brute force search:

- Let `n` represent the number of points in the dataset
- Let `d` represent the data dimensionality (how large a vector is)
- Let `k` be the number of neighbors considered for voting.

Given that, the time complexity for:

- finding one nearest neighbor is `O(n * d)`
- for finding k nearest neighbors, it is `O(n * d * k)`

This means the algorithm runs in linear time, which can be prohibitively slow for large datasets.

What strategies can we utilize to make similarity search faster? If we trade off some accuracy for speed, we can use Approximate Nearest Neighbor (ANN) algorithms, which can run in faster than linear time.

# Introduction to HNSW

HNSW is one of several ANN algorithms that have been developed to speed up similarity search. Originally proposed by Yu. A. Malkov and D. A. Yashunin in 2016, HNSW builds on the concept of Navigable Small World (NSW) graphs, which are a type of graph that allows for fast approximate nearest neighbor search.

Hierarchical NSW is a multi-layer graph structure that builds upon NSW, enable a logarithmic time complexity for nearest neighbor search. Performance is much improved compared to the previous state-of-the-art ANN algorithms.

</article>

<MultipleLayerGraphWithNodes client:load />
<DisplaySkipList client:only />

<article className="prose lg:prose-xl">

- Skip Lists improve on a traditional linked list by utilizing multiple sparse levels, with performance characteristics similar to a balanced tree
- NSW provides a way to build and search a graph that allows for fast approximate nearest neighbor search

- What is HNSW and why it's needed
- How HNSW improves upon KNN methods
  - ANN algorithms and their role in vector search
- Advantages of HNSW for vector database search
- Comparison with other search methods

7. Implementing HNSW from Scratch

- Understanding and implementing Skip List
- Understanding and implementing NSW
- Understanding and implementing HNS
- Testing and analyzing the performance of our HNSW implementation

</article>
