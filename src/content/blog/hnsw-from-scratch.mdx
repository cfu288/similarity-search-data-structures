---
title: "A Visual Guide to Higher Navigational Small World Graphs (HNSW) and How It Makes Vector Search Faster"
description: "Understanding HNSW by implementing a toy example from scratch"
pubDate: "March 20 2024"
heroImage: "/blog-placeholder-5.jpg"
---

import { DisplayNSWGraph } from "../../components/DisplayNSWGraph";
import { ExclamationTriangleIcon } from "@heroicons/react/20/solid";

<div className="border-l-4 border-blue-400 bg-blue-50 p-4 m-0">
  <div className="flex m-0 p-0 ">
    <div className="flex-shrink-0 m-0 p-0 ">
      <ExclamationTriangleIcon
        className="h-5 w-5 text-blue-400"
        aria-hidden="true"
      />
    </div>
    <div className="ml-3">
      <div className="ml-3">
        <div className="text-sm font-medium text-blue-800">{"Note"}</div>
        <div className="mt-2 text-sm text-blue-700">
          {
            "This article is a work in progress. The code examples and the visualizations are placeholders and will be updated soon."
          }
        </div>
      </div>
    </div>
  </div>
</div>

<article className="prose lg:prose-xl">

# Navigable Small World Demo

<DisplayNSWGraph client:load />

There has been a recent increase in interest in vector databases, primarily due to to the rise of Generative AI and the role that vector databases play
in building Retrieval-Augmented Generation (RAG) systems. The role of a vector databases is to _enable fast similarity search_
across a large number of documents. In a RAG system, vector databases are used to store and return relevant documents that can help a LLM answer a specific prompt.
This can enable a Large Language Model (LLM) to answer questions on things that it has not been trained on, or enable LLMs to provide more accurate and relevant responses.
RAG can help reduce hallucinations and improve the quality of the generated responses by providing relevant context to the LLM model.

The problem of similarity search is not new, and it has been a topic of interest in the field of information retrieval for a long time.
This article will focus on the problem of similarity/vector search and how recent advancements in Approximate Nearest Neighbor (ANN)
algorithms have improved the speed of similarity search in vector databases. More specifically, we'll focus on one such ANN algorithm called HNSW (Hierarchical Navigable Small World), which has been implemented in several vector databases/indexes (including PGVector, Faiss), and walk through building a toy implementation of HNSW from scratch.

If you find this article interesting and would like to see similar articles that dive through these other topics not touched in this post, let me know!

## Prerequisites

A basic understanding of the following topics would be helpful for this article:

- Basic data structures (Arrays, Linked Lists, Trees)
- Algorithms (Sorting, Searching)
- Time complexity and Big O notation
- Programming concepts (Classes, Functions, Variables)

## The Problem of Vector Search

The concept of similarity search is simple: given a vector, can we find the most similar vectors from a set of vectors?

But first, what even is a vector?

A vector is a mathematical object that has both magnitude and direction. In the context of vector search, a vector is a list of numbers that represent a point in a multi-dimensional space.

For example, a 2D vector can be represented as `[x, y]`, and a 3D vector can be represented as `[x, y, z]`. In the context of vector databases, a vector is a list of numbers that represent a document or an image.

Vectors can be used to represent documents, images, and other types of data. Ergo, vector search can be a way to implement similarity search and used to find similar documents, images, and other types of data.

For instance, a document can be depicted as a numerical vector, with each number signifying the occurrence frequency of a specific word in the document. Lately, generative AI applications have started to produce vectors from a LLM embedding of a document.

There are many ways we can generate vectors from documents, and the choice of method depends on the specific use case (out of scope for this article).

But once we actually have the vectors, how do we use these vectors to find similar documents?

## Hey Neighbour, Can I Borrow Some Sugar?

The concept of similarity search with vectors is simple. If we plot the vectors in a multi-dimensional space, similar vectors should cluster togetner and be in close proximity. This makes sense intuitively, similar things should be close together. If we wanted to find the most similar vectors to a given vector, we could simply calculate the distance between the given vector and all the other vectors and return the vectors with the smallest distance. Simple enough, right?

This is essentially what the k-Nearest Neighbors (kNN) algorithm does. It calculates the distance between the given vector and all the other vectors and returns the k vectors with the smallest distance.

### How do we measure distance between vectors?

There are various methods to calculate vector distances. The simplest is Euclidean distance, the straight-line distance in multi-dimensional space. However, in high-dimensional spaces, it's less effective due to the "curse of dimensionality", where noise can overshadow useful values. This issue can be partially mitigated with normalization and dimension weighting.

Another common distance metric is the Cosine similarity, which measures the cosine of the angle between two vectors.

In our examples below, we use Euclidean distance for simplicity.

## Is kNN All You Need? (Limitations of Naive Similarity Search)

- The curse of dimensionality and its impact on similarity search
- Naive kNN brute force
  - n: number of points in the training dataset
  - d: data dimensionality
  - k: number of neighbors that we consider for voting
  - Time complexity of finding one nearest neighbor: O(n \* d)
  - Time complexity of finding k nearest neighbors: O(n \* d \* k)

Despite the simplicity and intuitiveness of the k-Nearest Neighbors (kNN) algorithm, it has its limitations - primarily due to poor performance in high-dimensional and large-scale datasets.

To touch on algorithmic complexity in the naive kNN brute force approach:

- 'n' represents the number of points in the training dataset
- 'd' stands for data dimensionality
- 'k' is the number of neighbors considered for voting. The time complexity for finding
  one nearest neighbor is O(n _ d), and for finding k nearest neighbors, it is O(n _ d \* k). This means the algorithm
  essentially runs in linear time, which can be prohibitively slow for large datasets.

What strategies can we utilize to make similarity search faster?

## Introduction to HNSW

- What is HNSW and why it's needed
- How HNSW improves upon kNN methods
  - ANN algorithms and their role in vector search
- Advantages of HNSW for vector database search
- Comparison with other search methods

7. Implementing HNSW from Scratch

- Understanding and implementing Skip List
- Understanding and implementing NSW
- Understanding and implementing HNSW
- Testing and analyzing the performance of our HNSW implementation

</article>
