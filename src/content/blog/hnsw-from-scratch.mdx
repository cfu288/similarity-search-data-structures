---
title: "A Visual Guide to Higher Navigational Small World Graphs (HNSW) and How It Makes Vector Search Faster"
description: "Understanding HNSW by implementing a toy example from scratch"
pubDate: "March 20 2024"
heroImage: "/blog-placeholder-5.jpg"
---

import { DisplayNSWGraph } from "../../components/DisplayNSWGraph";
import { NoteDirective, WarnDirective } from "../../components/NoteDirective";
import { GraphNode } from "../../lib/navigable-small-world/graph-node";
import { SimpleNodesOnGraph } from "../../components/SimpleNodesOnGraph";

<WarnDirective text="This article is a work in progress. The code examples and the visualizations are placeholders and will be updated soon." />

<DisplayNSWGraph client:load autoRun={true} />

<article className="prose lg:prose-xl">

There has been a recent increase in interest in vector databases, primarily due to to the rise of Generative AI and the role that vector databases play
in building Retrieval-Augmented Generation (RAG) systems. The role of a vector databases is to _enable fast similarity search_
across a large number of documents. In a RAG system, vector databases are used to store and return relevant documents that can help a LLM answer a specific prompt.
This can enable a Large Language Model (LLM) to answer questions on things that it has not been trained on, or enable LLMs to provide more accurate and relevant responses.
RAG can help reduce hallucinations and improve the quality of the generated responses by providing relevant context to the LLM model.

The problem of similarity search is not new, and it has been a topic of interest in the field of information retrieval for a long time.
This article will focus on the problem of similarity/vector search and how recent advancements in Approximate Nearest Neighbor (ANN)
algorithms have improved the speed of similarity search in vector databases. More specifically, we'll focus on one such ANN algorithm called HNSW (Hierarchical Navigable Small World), which has been implemented in several vector databases/indexes (including PGVector, Faiss), and walk through building a toy implementation of HNSW from scratch.

The goal of this post is to help provide you a high level understanding of how HNSW works and how it can be used to speed up similarity search.

This may need to be split into multiple posts, as there are several topics to cover:

- Basics of similarity search and k-Nearest Neighbors (kNN)
- Introduction to Skip Lists and Navigable Small World (NSW) graphs with interactive demos and code samples
- Implementation of a toy HNSW with interactive visualizations

Even with the above topics, there is still so much to cover in the realm of vector search and ANN algorithms that we cannot squeeze into a single post. If you find this article interesting and would like to see similar articles that dive through these other topics not touched in this post, let me know!

## Prerequisites

A basic understanding of the following topics would be helpful for understanding this article:

- Basic data structures (Arrays, Linked Lists, Trees)
- Algorithms (Sorting, Searching)
- Time complexity and Big O notation
- Programming concepts (Classes, Functions, Variables)

## The Problem of Similarity Search

Similarity search, a process that identifies similar items within a large dataset, is a crucial component in various applications such as recommendation systems, search engines, and information retrieval systems. This process is often applied to diverse types of data, including text documents, images, audio files, and even gifs.

However, comparing these items directly can be complex. Unlike mathematics where you can just subtract two numbers to get the difference between them, there is no straightforward way to compare two text documents or images directly.
To simplify this, we can convert these items into numerical representations, known as vectors. This conversion allows us to perform comparisons in a more straightforward manner. For example, let's consider the following sentences:

1. "Ham Ham Ham Ham Ham"
2. "Spam Spam Spam Spam Spam"
3. "Ham Ham Spam Spam Spam"

We can compare these sentences by counting the frequency of each word, resulting in the following numerical representations:

1. "<span className="text-red-700">Ham Ham Ham Ham Ham</span>" -> <b>[<span className="text-red-700">5</span>, <span className="text-blue-700">0</span>]</b>
2. "<span className="text-blue-700">Spam Spam Spam Spam Spam</span>" -> <b>[<span className="text-red-700">0</span>, <span className="text-blue-700">5</span>]</b>
   {/* prettier-ignore */}
3. "<span className="text-red-700">Ham Ham </span><span className="text-blue-700">Spam Spam Spam</span>" -> <b>[<span className="text-red-700">2</span>,{" "}<span className="text-blue-700">3</span>]</b>

With these numerical vectors, we can now easily compare the sentences by calculating
the distances between the vectors. One common method to calculate the distance
between vectors is the Euclidean distance, which is the straight-line distance
between two points in a multi-dimensional space. For our two dimentional vectors we created aboce,
its easy for us to visualize the distance between the vectors by plotting the vectors on a 2D plot:

</article>

<SimpleNodesOnGraph client:load />

<article className="prose lg:prose-xl">

Its pretty easy to see that the distance between vectors 2 and 3 is less than the distance between vectors 1 and 3, which makes sense. Sentence 3 is more similar to sentence 2 than to sentence 1 because it has more "Spam" than "Ham".

There are many techniques to generate vectors from data. In the above example, we
used the bag-of-words method to generate a vector from a text document, where each
number in the vector represents the frequency of a particular word. More sophisticated
methods, such as Large Language Model (LLM) embeddings, are also used to generate vectors.

The choice of which method is used to generate a vector representation depends on the specific use case. Once we have the vectors, we can use them to find similar items.

</article>

<NoteDirective text="Vectors are mathematical objects with magnitude and direction. In vector search, a vector is a list of numbers representing a point in a multi-dimensional space. For example, a 2D vector can be represented as `[x, y]`, and a 3D vector as `[x, y, z]`. An n-dimensional vector can be represented as `[x1, x2, ..., xn]`." />

<article className="prose lg:prose-xl">

## Hey Neighbour, Can I Borrow Some Sugar?

As seen with the example above, the concept of similarity search with vectors is intuitive. If we plot the vectors in a multi-dimensional space, similar vectors should be in close proximity to each other for any given distance metric.

This is essentially what the k-Nearest Neighbors (kNN) algorithm does. It calculates the distance between the given vector and all the other vectors in the dataset and returns the k vectors with the smallest distance to the given vector.

### How do we measure distance between vectors?

There are various methods to calculate vector distances. The simplest is Euclidean distance (which we used above), the straight-line distance in multi-dimensional space. However, in high-dimensional spaces, it's less effective due to the "curse of dimensionality", where noise can overshadow useful values. This issue can be partially mitigated with normalization and dimension weighting.

Another common distance metric is the Cosine similarity, which measures the cosine of the angle between two vectors. This can be a highly performant metric for text data, as it is a fast calculation that ignores the magnitude of the vectors and focuses on the direction. It is also less sensitive to the curse of dimensionality.

In our examples below, we use Euclidean distance for simplicity.

## Is kNN All You Need?

Despite the simplicity and intuitiveness of the k-Nearest Neighbors (kNN) algorithm, it has its limitations - primarily due to poor performance in high-dimensional and large-scale datasets.

To touch on algorithmic complexity in the naive kNN brute force search:

- Let `n` represent the number of points in the dataset
- Let `d` represent the data dimensionality (how large a vector is)
- Let `k` be the number of neighbors considered for voting.

Given that, the time complexity for:

- finding one nearest neighbor is `O(n * d)`
- for finding k nearest neighbors, it is `O(n * d * k)`

This means the algorithm essentially runs in linear time, which can be prohibitively slow for large datasets.

What strategies can we utilize to make similarity search faster? If we trade off some accuracy for speed, we can use Approximate Nearest Neighbor (ANN) algorithms.

## Introduction to HNSW

- Skip Lists improve on a traditional linked list by utilizing multiple sparse levels, with performance characteristics similar to a balanced tree
- NSW provides a way to build and search a graph that allows for fast approximate nearest neighbor search

- What is HNSW and why it's needed
- How HNSW improves upon kNN methods
  - ANN algorithms and their role in vector search
- Advantages of HNSW for vector database search
- Comparison with other search methods

7. Implementing HNSW from Scratch

- Understanding and implementing Skip List
- Understanding and implementing NSW
- Understanding and implementing HNSW
- Testing and analyzing the performance of our HNSW implementation

</article>
