---
title: "A Visual Guide to Higher Navigational Small World Graphs (HNSW) and How It Makes Vector Search Faster"
description: "Understanding HNSW by implementing a toy example from scratch"
pubDate: "March 20 2024"
heroImage: "/HNSW.jpg"
---

import { DisplayNSWGraph } from "../../components/DisplayNSWGraph";
import {
  NoteDirective,
  WarnDirective,
  Directive,
} from "../../components/NoteDirective";
import { GraphNode } from "../../lib/navigable-small-world/graph-node";
import { SimpleNodesOnGraph } from "../../components/SimpleNodesOnGraph";
import { MultipleLayerGraphWithNodes } from "../../components/MultipleLayerGraphWithNodes";
import { DisplaySkipList } from "../../components/DisplaySkipList";

<WarnDirective text="This article is a work in progress. The code examples and the visualizations are placeholders and will be updated soon." />

<DisplayNSWGraph client:load autoRun={true} />

<article className="prose lg:prose-xl">

This article aims to give a high-level overview of the Hierarchical Navigable Small World (HNSW) algorithm and how it can be used
to speed up similarity search. While it appears to be a complex algorithm at first, understanding it can be much simpler if we explore the
fundamental concepts it is built on. As you'll see, HNSW is just one complex algorithm that is the culmination of several
more straightforward concepts combined. Understanding these simpler concepts on a conceptual makes understanding HNSW straightforward.

In this post, we will explain this algorithm from a fundamentals-first perspective, work through each of the algorithms it was built on,
like skip lists and navigatable small worlds, and use interactive visualizations to walk through how they work. In a later post, we'll
build a toy implementation of each of the algorithms from scratch.

Many of my posts target medical professionals with a minimal technical background, but who are interested in tech, so we'll avoid
digging too deep into the math and using mathematical notation. However, this specific post may require some basic understanding of topics
in computer science like data structures (Arrays, Linked Lists, Trees) and time complexity (Big O notation).

# Why is HNSW Important?

There has been a recent increase in interest in vector search and databases, primarily due to the rise of Generative AI and the role vector databases play
in building Retrieval-Augmented Generation (RAG) systems. The role of a vector database is to _enable fast similarity search_
across a large number of documents. In a RAG system, vector databases store and return relevant documents that can help an LLM answer a specific prompt.
By leveraging vector databases, a large language model (LLM) can transcend its training limitations. It can now respond to queries on topics it hasn't been specifically trained on or deliver more precise and contextually relevant responses.
Retrieval-Augmented Generation (RAG) systems are a game-changer. They can significantly mitigate the issue of hallucinations and enhance the quality of responses generated by an LLM. This is achieved by providing the LLM with relevant context, thereby improving its understanding and response accuracy.

The problem of similarity search in computer science is not new; it has long been a topic of interest in the field of information retrieval.
This article will focus on the problem of similarity/vector search and how recent advancements in Approximate Nearest Neighbor (ANN)
algorithms have improved the speed of similarity search in vector databases. More specifically, we'll focus on one such ANN algorithm called HNSW (Hierarchical Navigable Small World), which has been implemented in several vector databases/indexes (including PGVector, Faiss), and walk through building a toy implementation of HNSW from scratch.

# What to Expect

This post may need to be split into multiple posts, as there are several topics to cover:

- Basics of similarity search and k-nearest neighbors (KNN)
- Introduction to Skip Lists and Navigable Small World (NSW) graphs with interactive demos and code samples
- Implementation of a toy HNSW with interactive visualizations

Even with the above topics, there is still so much to cover in the realm of similarity search that we cannot squeeze into a single post. If you find this article interesting and would like to see similar articles that dive through these other topics not touched on in this post, let me know!

## The Problem of Similarity Search

Similarity search, a process that identifies similar items within a large dataset, is crucial in various applications such as recommendation systems, search engines, and information retrieval systems. This process is often applied to diverse data types, including text documents, images, audio files, and even gifs.

However, comparing these items directly can be complex. Unlike numbers, where you can subtract two to get the difference between them, there is no straightforward way to compare two text documents or images. To remedy this, we can convert these items into their numerical representations, vectors. This conversion allows us to perform mathematical comparisons more straightforwardly.

For example, let's consider the following sentences:

1. "Ham Ham Ham Ham Ham"
2. "Spam Spam Spam Spam Spam"
3. "Ham Ham Spam Spam Spam"

We can compare these sentences by counting the frequency of each word, resulting in the following numerical representations:

1. "<span className="text-red-700">Ham Ham Ham Ham Ham</span>" -> <b>[<span className="text-red-700">5</span>, <span className="text-blue-700">0</span>]</b>
2. "<span className="text-blue-700">Spam Spam Spam Spam Spam</span>" -> <b>[<span className="text-red-700">0</span>, <span className="text-blue-700">5</span>]</b>
   {/* prettier-ignore */}
3. "<span className="text-red-700">Ham Ham </span><span className="text-blue-700">Spam Spam Spam</span>" -> <b>[<span className="text-red-700">2</span>,{" "}<span className="text-blue-700">3</span>]</b>

These numerical vector representations are just a list of numbers, which enables us to now easily compare the sentences by calculating
the distances between them.

</article>

<Directive text="In vector search, a vector is a list of numbers representing a point in a multi-dimensional space. A 2D vector can be represented as `[x, y]`, and a 3D vector as `[x, y, z]`. An n-dimensional vector can be represented as `[x1, x2, ..., xn]`." />

<article className="prose lg:prose-xl">

One standard method to calculate the distance
between vectors is the Euclidean distance, which is the straight-line distance
between two points in a multi-dimensional space. For the two-dimensional vectors we created above,
it's easy for us to visualize the distance between the vectors by plotting the vectors on a 2D plot:

</article>

<SimpleNodesOnGraph client:load />

<article className="prose lg:prose-xl">

It's easy to visually see that the distance between vectors 2 and 3 is less than the distance between vectors 1 and 3, which makes sense. Sentence 3 is more similar to sentence 2 than to sentence 1 because it has more instances of "Spam" than "Ham."

While the Euclidian distance is a simple and intuitive way to measure distance between vectors, it is not always the best choice. High-dimensional spaces can suffer from the "curse of dimensionality," where noisy or unrelated values in the vector can overshadow valuable values, skewing the distance between vectors. This issue can be partially mitigated with data normalization and dimension weighting. However, other distance metrics are less susceptible to these issues.

One such distance metric that is commonly used is Cosine similarity. This can be a highly performant metric, as it is a fast calculation that ignores the magnitude of the vectors and focuses on the direction.

In all of our examples on this page, we use Euclidean distance to simplify visualization.

### Methods to Generate Vectors from Data

There are many techniques to generate vectors from data.

In the above example, we
used the bag-of-words method to generate a vector from a text document, where each
number in the vector represents the frequency of a particular word.

More sophisticated
methods, such as embeddings generated from Large Language Models (LLM) as vectors, are also used.

The choice of which method is used to generate a vector representation depends on the specific use case. Once we have the vectors, we can use them for similarity search.

### Hey Neighbour, Can I Borrow Some Sugar?

As the example above shows, a similarity search with vectors is intuitive. Suppose we plot the vectors in a multi-dimensional space. In that case, similar vectors should be close to each other for any distance metric.

The k-nearest neighbors (KNN) algorithm does this. It calculates the distance between the given vector and all the other vectors in the dataset and returns the k vectors with the smallest distance to the given vector.

Below is an illustration of KNN (with `k` = 1), where we have vector 0 with a vector of [2,3] that we want to find the closest vector to from the following set:

</article>

<SimpleNodesOnGraph
  client:load
  nodes={[
    new GraphNode(1, [6, 1]),
    new GraphNode(2, [5, 6]),
    new GraphNode(3, [5, 4]),
    new GraphNode(4, [1, 5]),
    new GraphNode(5, [0, 1]),
  ]}
  targetNode={new GraphNode(0, [2, 3])}
  k={1}
/>

<article className="prose lg:prose-xl">

We can see that the vector 0 is closest to vector 2, so the KNN algorithm would return vector 2 as the nearest neighbor.

### Is KNN All You Need?

Despite its simplicity and intuitiveness, the k-nearest neighbors (KNN) algorithm has limitations, primarily due to poor performance in high-dimensional and large-scale datasets.

To touch on algorithmic complexity in the naive KNN brute force search:

- Let `n` represent the number of points in the dataset
- Let `d` represent the data dimensionality (how large a vector is)
- Let `k` be the number of neighbors considered for voting.

Given that, the time complexity for:

- finding one nearest neighbor is `O(n * d)`
- for finding k nearest neighbors, it is `O(n * d * k)`

This means the algorithm runs in linear time, which can be prohibitively slow for large datasets.

What strategies can we utilize to make similarity search faster? If we trade off some accuracy for speed, we can use Approximate Nearest Neighbor (ANN) algorithms.

## Introduction to HNSW

</article>

<MultipleLayerGraphWithNodes client:load />
<DisplaySkipList client:only />

<article className="prose lg:prose-xl">

- Skip Lists improve on a traditional linked list by utilizing multiple sparse levels, with performance characteristics similar to a balanced tree
- NSW provides a way to build and search a graph that allows for fast approximate nearest neighbor search

- What is HNSW and why it's needed
- How HNSW improves upon KNN methods
  - ANN algorithms and their role in vector search
- Advantages of HNSW for vector database search
- Comparison with other search methods

7. Implementing HNSW from Scratch

- Understanding and implementing Skip List
- Understanding and implementing NSW
- Understanding and implementing HNS
- Testing and analyzing the performance of our HNSW implementation

</article>
