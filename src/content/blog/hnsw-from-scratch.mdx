---
title: "A Visual Guide to Higher Navigational Small World Graphs (HNSW) and How It Makes Vector Search Faster"
description: "Understanding HNSW by implementing a toy example from scratch"
pubDate: "March 20 2024"
heroImage: "/blog-placeholder-5.jpg"
---

import { DisplayNSWGraph } from "../../components/DisplayNSWGraph";
import {
  NoteDirective,
  WarnDirective,
  Directive,
} from "../../components/NoteDirective";
import { GraphNode } from "../../lib/navigable-small-world/graph-node";
import { SimpleNodesOnGraph } from "../../components/SimpleNodesOnGraph";
import { MultipleLayerGraphWithNodes } from "../../components/MultipleLayerGraphWithNodes";

<WarnDirective text="This article is a work in progress. The code examples and the visualizations are placeholders and will be updated soon." />

<DisplayNSWGraph client:load autoRun={true} />

<article className="prose lg:prose-xl">

There has been a recent increase in interest in vector search and databases, primarily due to to the rise of Generative AI and the role that vector databases play
in building Retrieval-Augmented Generation (RAG) systems. The role of a vector databases is to _enable fast similarity search_
across a large number of documents. In a RAG system, vector databases are used to store and return relevant documents that can help a LLM answer a specific prompt.
This can enable a Large Language Model (LLM) to answer questions on things that it has not been trained on, or enable LLMs to provide more accurate and relevant responses.
RAG can help reduce hallucinations and improve the quality of the generated responses by providing relevant context to the LLM model.

The problem of similarity search is not new, and it has been a topic of interest in the field of information retrieval for a long time.
This article will focus on the problem of similarity/vector search and how recent advancements in Approximate Nearest Neighbor (ANN)
algorithms have improved the speed of similarity search in vector databases. More specifically, we'll focus on one such ANN algorithm called HNSW (Hierarchical Navigable Small World), which has been implemented in several vector databases/indexes (including PGVector, Faiss), and walk through building a toy implementation of HNSW from scratch.

The goal of this post is to help provide you a high level understanding of how HNSW works and how it can be used to speed up similarity search.

This post may need to be split into multiple posts, as there are several topics to cover:

- Basics of similarity search and k-nearest neighbors (KNN)
- Introduction to Skip Lists and Navigable Small World (NSW) graphs with interactive demos and code samples
- Implementation of a toy HNSW with interactive visualizations

Even with the above topics, there is still so much to cover in the realm of vector search and ANN algorithms that we cannot squeeze into a single post. If you find this article interesting and would like to see similar articles that dive through these other topics not touched in this post, let me know!

## Prerequisites

While this article is aimed at beginners, it assumes a basic understanding of programming concepts and data structures. A basic understanding of the following topics would be helpful for understanding this article:

- Basic data structures and algorithms (Arrays, Linked Lists, Trees)
- Time complexity and Big O notation

## The Problem of Similarity Search

Similarity search, a process that identifies similar items within a large dataset, is a crucial component in various applications such as recommendation systems, search engines, and information retrieval systems. This process is often applied to diverse types of data, including text documents, images, audio files, and even gifs.

However, comparing these items directly can be complex. Unlike numbers where you can just subtract two numbers to get the difference between them, there is no straightforward way to compare two text documents or images. To remedy this, we can convert these items into their numerical representations, known as vectors. This conversion allows us to perform mathematical comparisons in a more straightforward manner.

For example, let's consider the following sentences:

1. "Ham Ham Ham Ham Ham"
2. "Spam Spam Spam Spam Spam"
3. "Ham Ham Spam Spam Spam"

We can compare these sentences by counting the frequency of each word, resulting in the following numerical representations:

1. "<span className="text-red-700">Ham Ham Ham Ham Ham</span>" -> <b>[<span className="text-red-700">5</span>, <span className="text-blue-700">0</span>]</b>
2. "<span className="text-blue-700">Spam Spam Spam Spam Spam</span>" -> <b>[<span className="text-red-700">0</span>, <span className="text-blue-700">5</span>]</b>
   {/* prettier-ignore */}
3. "<span className="text-red-700">Ham Ham </span><span className="text-blue-700">Spam Spam Spam</span>" -> <b>[<span className="text-red-700">2</span>,{" "}<span className="text-blue-700">3</span>]</b>

These numerical vector representations are just a list of numbers, which enables us to now easily compare the sentences by calculating
the distances between them.

</article>

<Directive text="In vector search, a vector is a list of numbers representing a point in a multi-dimensional space. A 2D vector can be represented as `[x, y]`, and a 3D vector as `[x, y, z]`. An n-dimensional vector can be represented as `[x1, x2, ..., xn]`." />

<article className="prose lg:prose-xl">

One common method to calculate the distance
between vectors is the Euclidean distance, which is the straight-line distance
between two points in a multi-dimensional space. For our two dimentional vectors we created above,
its easy for us to visualize the distance between the vectors by plotting the vectors on a 2D plot:

</article>

<SimpleNodesOnGraph client:load />

<article className="prose lg:prose-xl">

Its pretty easy to visually see that the distance between vectors 2 and 3 is less than the distance between vectors 1 and 3, which makes sense. Sentence 3 is more similar to sentence 2 than to sentence 1 because it has more instances of "Spam" than "Ham".

While the euclidian distance is a simple and intuitive way to measure distance between vectors, it is not always the best choice. In high-dimensional spaces, it can suffer from the "curse of dimensionality", where noisy values in the vector can overshadow useful values. This issue can be partially mitigated with data normalization and dimension weighting, however there are other distance metrics that are less susceptible to these issues.

Another distance metric that is commonly used in text data is the Cosine similarity. This can be a highly performant metric, as it is a fast calculation that ignores the magnitude of the vectors and focuses on the direction.

In all of our examples on this page, we use Euclidean distance for simplicity of visualization.

### Methods to Generate Vectors from Data

There are many techniques to generate vectors from data.

In the above example, we
used the bag-of-words method to generate a vector from a text document, where each
number in the vector represents the frequency of a particular word.

More sophisticated
methods, such as the use of embeddings generated from Large Language Models (LLM) as vecotrs are also used.

The choice of which method is used to generate a vector representation depends on the specific use case. Once we have the vectors, we can use them to conduct similarity search.

## Hey Neighbour, Can I Borrow Some Sugar?

As seen with the example above, the concept of similarity search with vectors is intuitive. If we plot the vectors in a multi-dimensional space, similar vectors should be in close proximity to each other for any given distance metric.

This is essentially what the k-nearest neighbors (KNN) algorithm does. It calculates the distance between the given vector and all the other vectors in the dataset and returns the k vectors with the smallest distance to the given vector.

Below is an illustration of KNN, where we have a vector 0 with value of [2,3]. We want to find the closest vector (`k` = 1) in the set:

</article>

<SimpleNodesOnGraph
  client:load
  nodes={[
    new GraphNode(5, [0, 1]),
    new GraphNode(4, [1, 5]),
    new GraphNode(3, [5, 4]),
    new GraphNode(2, [5, 6]),
    new GraphNode(1, [6, 1]),

]}
targetNode={new GraphNode(0, [2, 3])}
k={1}
/>

<article className="prose lg:prose-xl">

We can see that the vector 0 is closest to vector 2, so the KNN algorithm would return vector 2 as the nearest neighbor.

## Is KNN All You Need?

Despite the simplicity and intuitiveness of the k-nearest neighbors (KNN) algorithm, it has its limitations - primarily due to poor performance in high-dimensional and large-scale datasets.

To touch on algorithmic complexity in the naive KNN brute force search:

- Let `n` represent the number of points in the dataset
- Let `d` represent the data dimensionality (how large a vector is)
- Let `k` be the number of neighbors considered for voting.

Given that, the time complexity for:

- finding one nearest neighbor is `O(n * d)`
- for finding k nearest neighbors, it is `O(n * d * k)`

This means the algorithm essentially runs in linear time, which can be prohibitively slow for large datasets.

What strategies can we utilize to make similarity search faster? If we trade off some accuracy for speed, we can use Approximate Nearest Neighbor (ANN) algorithms.

## Introduction to HNSW

</article>

<MultipleLayerGraphWithNodes client:load />

<article className="prose lg:prose-xl">

- Skip Lists improve on a traditional linked list by utilizing multiple sparse levels, with performance characteristics similar to a balanced tree
- NSW provides a way to build and search a graph that allows for fast approximate nearest neighbor search

- What is HNSW and why it's needed
- How HNSW improves upon KNN methods
  - ANN algorithms and their role in vector search
- Advantages of HNSW for vector database search
- Comparison with other search methods

7. Implementing HNSW from Scratch

- Understanding and implementing Skip List
- Understanding and implementing NSW
- Understanding and implementing HNS
- Testing and analyzing the performance of our HNSW implementation

</article>
